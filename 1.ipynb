{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you need to remember: \n",
    "* np.exp(x) works for any np.array x and applies the exponential function to every coordinate \n",
    "* the sigmoid function and its gradient \n",
    "* image2vector is commonly used in deep learning \n",
    "* np.reshape is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. \n",
    "* numpy has efficient built-in functions \n",
    "* broadcasting is extremely useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# GRADED FUNCTION: basic_sigmoid import math\n",
    "def basic_sigmoid(x): \n",
    "    \"\"\" \n",
    "    Compute sigmoid of x.\n",
    "    Arguments: x -- A scalar\n",
    "    Return:\n",
    "    s -- sigmoid(x) \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (￿ 1 line of code) \n",
    "    s = 1/(1+math.exp(-x)) \n",
    "    ### END CODE HERE ###\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.71828183   7.3890561   20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# example of np.exp \n",
    "x = np.array([1, 2, 3]) \n",
    "print(np.exp(x)) \n",
    "# result is (exp(1), exp(2), exp(3))\n",
    "#output [ 2.71828183 7.3890561 20.08553692] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation \n",
    "x = np.array([1, 2, 3]) \n",
    "print (x + 3)\n",
    "#output [4 5 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid \n",
    "import numpy as np \n",
    "# this means you can access numpy functions by writing np.function() instead of numpy.function(), \n",
    "def sigmoid(x):\n",
    "    \"\"\" \n",
    "    Compute the sigmoid of x\n",
    "    Arguments: \n",
    "    x -- A scalar or numpy array of any size\n",
    "    Return: \n",
    "    s -- sigmoid(x) \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (￿ 1 line of code) \n",
    "    s = 1/(1+np.exp(-x)) \n",
    "    ### END CODE HERE ###\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73105858,  0.88079708,  0.95257413])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3]) \n",
    "sigmoid(x)\n",
    "#output array([ 0.73105858, 0.88079708, 0.95257413])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative \n",
    "def sigmoid_derivative(x): \n",
    "    \"\"\" \n",
    "    Compute the gradient (also called the slope or derivative) \n",
    "    of the sigmoid function with respect to its input x., \n",
    "    → You can store the output of the sigmoid function into \n",
    "    variables and then use it to calculate the gradient., →\n",
    "    Arguments: \n",
    "    x -- A scalar or numpy array\n",
    "    Return: \n",
    "    ds -- Your computed gradient. \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (￿ 2 lines of code) \n",
    "    s = 1/(1+np.exp(-x)) \n",
    "    ds = s*(1-s) \n",
    "    ### END CODE HERE ###\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector \n",
    "def image2vector(image): \n",
    "    \"\"\" \n",
    "    Argument: \n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    Returns: \n",
    "    v -- a vector of shape (length*height*depth, 1) \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (￿ 1 line of code) \n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1) \n",
    "    ### END CODE HERE ###\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalizeRows \n",
    "def normalizeRows(x): \n",
    "    \"\"\" \n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length)., →\n",
    "    Argument: \n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x., → \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (￿ 2 lines of code) \n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True), → \n",
    "    x_norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    # Divide x by its norm. \n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    print(x_norm.shape)\n",
    "    print(x_norm)\n",
    "    x = x/x_norm \n",
    "    ### END CODE HERE ###\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[0 3 4]\n",
      " [1 6 4]]\n",
      "(2, 1)\n",
      "[[ 5.        ]\n",
      " [ 7.28010989]]\n",
      "normalizeRows(x) = [[ 0.          0.6         0.8       ]\n",
      " [ 0.13736056  0.82416338  0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3, 4], [1, 6, 4]]) \n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))\n",
    "#output normalizeRows(x) = [[ 0. 0.6 0.8 ] [ 0.13736056 0.82416338 0.54944226]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax \n",
    "def softmax(x): \n",
    "    \"\"\"\n",
    "    Calculates the softmax for each row of the input x.\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m)., →\n",
    "    Argument: \n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "    Returns: \n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m) \"\"\"\n",
    "    # Apply exp() element-wise to x. Use np.exp(...). \n",
    "    x_exp = np.exp(x)\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True)., → \n",
    "    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting., → \n",
    "    print(x_sum)\n",
    "    print(x_exp)\n",
    "    s = x_exp/x_sum\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  75.68368696]\n",
      " [ 460.74522535]]\n",
      "[[   1.           20.08553692   54.59815003]\n",
      " [   2.71828183  403.42879349   54.59815003]]\n",
      "softmax(x) = [[ 0.01321289  0.26538793  0.72139918]\n",
      " [ 0.00589975  0.8756006   0.11849965]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3, 4], [1, 6, 4]]) \n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.0ms\n",
      "outer = [[ 81.  18.  18.  81.   0.  81.  18.  45.   0.   0.  81.  18.  45.   0.\n",
      "    0.]\n",
      " [ 18.   4.   4.  18.   0.  18.   4.  10.   0.   0.  18.   4.  10.   0.\n",
      "    0.]\n",
      " [ 45.  10.  10.  45.   0.  45.  10.  25.   0.   0.  45.  10.  25.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [ 63.  14.  14.  63.   0.  63.  14.  35.   0.   0.  63.  14.  35.   0.\n",
      "    0.]\n",
      " [ 45.  10.  10.  45.   0.  45.  10.  25.   0.   0.  45.  10.  25.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [ 81.  18.  18.  81.   0.  81.  18.  45.   0.   0.  81.  18.  45.   0.\n",
      "    0.]\n",
      " [ 18.   4.   4.  18.   0.  18.   4.  10.   0.   0.  18.   4.  10.   0.\n",
      "    0.]\n",
      " [ 45.  10.  10.  45.   0.  45.  10.  25.   0.   0.  45.  10.  25.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]]\n",
      " ----- Computation time = 0.0ms\n",
      "278.0\n",
      "elementwise multiplication = [ 81.   4.  10.   0.   0.  63.  10.   0.   0.   0.  81.   4.  25.   0.   0.]\n",
      " ----Computation time = 0.0ms\n",
      "[ 0.  0.  0.]\n",
      "0\n",
      "1\n",
      "2\n",
      "gdot = [ 17.5859165   24.00058824  25.88400647]\n",
      " ----- Computation time = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0] \n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ### \n",
    "#dot矩阵乘法：对应相乘再求和\n",
    "tic = time.process_time() \n",
    "dot = 0 \n",
    "for i in range(len(x1)): \n",
    "    dot+= x1[i]*x2[i] \n",
    "toc = time.process_time() \n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + \n",
    "       str(1000*(toc - tic)) + \"ms\"),\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ### \n",
    "tic = time.process_time() \n",
    "#outer是a的第一个元素跟b的每一个元素相乘作为第一行，第二个元素跟b的每一个元素相乘作为第二个元素...\n",
    "outer = np.zeros((len(x1),len(x2))) \n",
    "    # we create a len(x1)*len(x2) matrix with only zeros, \n",
    "for i in range(len(x1)): \n",
    "        for j in range(len(x2)): \n",
    "            outer[i,j] = x1[i]*x2[j] \n",
    "toc = time.process_time() \n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + \n",
    "                   str(1000*(toc - tic)) + \"ms\"), \n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ### \n",
    "tic = time.process_time()\n",
    "#multiply是对应位相乘(但不求和,sum(np.multiply(x1,x2))=np.dot(x1,x2))\n",
    "mul = np.zeros(len(x1)) \n",
    "for i in range(len(x1)): \n",
    "    mul[i] = x1[i]*x2[i] \n",
    "toc = time.process_time() \n",
    "print(sum(mul))\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----Computation time = \" + \n",
    "       str(1000*(toc - tic)) + \"ms\"),\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ### \n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array \n",
    "tic = time.process_time() \n",
    "#shape函数是numpy.core.fromnumeric中的函数，它的功能是读取矩阵的长度\n",
    "#比如shape[0]就是读取矩阵第一维度的长度。\n",
    "gdot = np.zeros(W.shape[0])\n",
    "print(gdot)\n",
    "#W的第一行和x1做dot运算，即相乘再求和，作为第一个元素。每一行乘完求和为一个元素。\n",
    "for i in range(W.shape[0]): \n",
    "    print(i)\n",
    "    for j in range(len(x1)): \n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time() \n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.0ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.0ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----Computation time = 0.0ms\n",
      "gdot = [ 17.5859165   24.00058824  25.88400647]\n",
      " ----- Computation time = 0.0ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25.884006473149245"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### VECTORIZED DOT PRODUCT OF VECTORS ### \n",
    "tic = time.process_time() \n",
    "#dot矩阵乘法：对应相乘再求和\n",
    "dot = np.dot(x1,x2) \n",
    "toc = time.process_time() \n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + \n",
    "       str(1000*(toc - tic)) + \"ms\"),\n",
    "### VECTORIZED OUTER PRODUCT ### \n",
    "tic = time.process_time() \n",
    "#outer是a的第一个元素跟b的每一个元素相乘作为第一行，第二个元素跟b的每一个元素相乘作为第二个元素...\n",
    "outer = np.outer(x1,x2) \n",
    "toc = time.process_time() \n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + \n",
    "       str(1000*(toc - tic)) + \"ms\"),\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ### \n",
    "tic = time.process_time() \n",
    "#multiply是对应位相乘(但不求和,sum(np.multiply(x1,x2))=np.dot(x1,x2))\n",
    "mul = np.multiply(x1,x2) \n",
    "toc = time.process_time() \n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----Computation time = \" + \n",
    "       str(1000*(toc - tic)) + \"ms\"),\n",
    "### VECTORIZED GENERAL DOT PRODUCT ### \n",
    "tic = time.process_time() \n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time() \n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + \n",
    "       str(1000*(toc - tic)) + \"ms\"), \n",
    "#print( np.sum(np.multiply(W,x1),axis = 1,keepdims = True))\n",
    "#np.max(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 loss is defined as:\n",
    "* L1(ˆy,y) =∑ |y(i) − ˆ y(i)| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1 \n",
    "def L1(yhat, y): \n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    yhat -- vector of size m (predicted labels) \n",
    "    y -- vector of size m (true labels)\n",
    "    Returns: \n",
    "    loss -- the value of the L1 loss function defined above \n",
    "    \"\"\"\n",
    "    loss = sum(abs(y-yhat))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9]) \n",
    "y = np.array([1, 0, 0, 1, 1]) \n",
    "print(\"L1 = \" + str(L1(yhat,y))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 loss is defined as L2(ˆy,y) = ∑(y(i)-^y(i))^2\n",
    "* then “np.dot(x,x)” =∑x(i)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2 \n",
    "def L2(yhat, y): \n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    yhat -- vector of size m (predicted labels) \n",
    "    y -- vector of size m (true labels)\n",
    "    Returns: \n",
    "    loss -- the value of the L2 loss function defined above \n",
    "    \"\"\"\n",
    "    loss = np.dot(y-yhat,y-yhat)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9]) \n",
    "y = np.array([1, 0, 0, 1, 1]) \n",
    "print(\"L2 = \" + str(L2(yhat,y))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
